{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%pip install ydata-profiling","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from ydata_profiling import ProfileReport\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom scipy.stats import entropy\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Understanding the Problem and Objective:\nBefore diving into the data, I needed understand the problem I was trying to solve and the goals of this analysis. \n\n## About Data\nFor this Health Insurance data,this dataset is about an Insurance company that has provided Health Insurance to its customers. \n\n## Goal or Objective at  Hand\nI need to build a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.","metadata":{"editable":false}},{"cell_type":"code","source":"# Importing libraries and Loading the Dataset:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\ntrain_dataset = pd.read_csv(\"/kaggle/input/playground-series-s4e7/train.csv\")\ntest_dataset = pd.read_csv(\"/kaggle/input/playground-series-s4e7/test.csv\")","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking a look at the structure of the Dataset\ntrain_df = train_dataset.sample(frac=0.4, random_state=42)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the Duplicate and Missing Values in the Dataset\ntrain_df.duplicated().sum()\ntrain_df.drop_duplicates(inplace=True)\ntrain_df.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = train_df.isnull().sum()\nmissing_values","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis(EDA)","metadata":{"editable":false}},{"cell_type":"code","source":"#profile = ProfileReport(train_df, title=\"Profiling Report\")","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#profile.to_notebook_iframe()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Preprocessing","metadata":{"editable":false}},{"cell_type":"code","source":"train_df.head(10)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert categorical variables\ncategorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\ntrain_df = pd.get_dummies(train_df, columns=categorical_features, drop_first=True)\n\n# Normalize/Standardize numerical features\nnumerical_features = ['Annual_Premium', 'Vintage']\nscaler = StandardScaler()\ntrain_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\n\ntrain_df.head(10)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Feature Engineering","metadata":{"editable":false}},{"cell_type":"code","source":"# Binning Age\ntrain_df['Age_Bin'] = pd.cut(train_df['Age'], bins=[0, 30, 40, 50, 60, 70, 80], labels=[1, 2, 3, 4, 5, 6])\n\n# Create Insurance History feature\ntrain_df['Insurance_History'] = train_df['Previously_Insured'] * train_df['Vintage']\n\n# Use quantile-based binning for normalized Vintage values\nquantile_transformer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile', subsample=None)\ntrain_df['Vintage_Bin'] = quantile_transformer.fit_transform(train_df[['Vintage']])\n\n# Create a new feature combining vehicle damage status and vehicle age\ntrain_df['Vehicle_Damage_Age'] = train_df['Vehicle_Damage_Yes'] * (\n    train_df['Vehicle_Age_< 1 Year'] * 1 + \n    train_df['Vehicle_Age_> 2 Years'] * 2\n)\n\n# Calculate entropy for Policy_Sales_Channel\nsales_channel_entropy = train_df['Policy_Sales_Channel'].value_counts(normalize=True)\ntrain_df['Policy_Sales_Channel_Entropy'] = entropy(sales_channel_entropy)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\ntrain_df.isnull().sum()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Prepare data for Modeling","metadata":{"editable":false}},{"cell_type":"code","source":"# Convert bool variables\ncategorical_bool_features = ['Gender_Male', 'Vehicle_Age_< 1 Year', 'Vehicle_Age_> 2 Years', 'Vehicle_Damage_Yes']\ntrain_df[categorical_bool_features] = train_df[categorical_bool_features].astype(int)\n\n\n# Fill missing values for each column with the mode\nfor column in train_df.columns:\n    mode_value = train_df[column].mode()[0]\n    train_df.loc[:, column] = train_df[column].fillna(mode_value)\n\n# Define the features and target variable\nX = train_df.drop(['Age', 'Response'], axis=1)\ny = train_df['Response']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(15)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import KFold\n\n# Initialize the models\nmodels = {\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n}\n\n# Define the K-fold cross-validation object\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Train and evaluate the models\nmodel_scores = {}\nfor model_name, model in models.items():\n    # Initialize the AUC-ROC scores\n    auc_scores = []\n\n    # Perform K-fold cross-validation\n    for train_index, val_index in kfold.split(X_train):\n        X_train_fold = X_train.iloc[train_index]\n        y_train_fold = y_train.iloc[train_index]\n        X_val_fold = X_train.iloc[val_index]\n        y_val_fold = y_train.iloc[val_index]\n\n        # Train the model on the current fold\n        model.fit(X_train_fold, y_train_fold)\n\n        # Make predictions on the validation set\n        y_pred_proba_fold = model.predict_proba(X_val_fold)[:, 1]\n\n        # Evaluate the model on the current fold\n        auc = roc_auc_score(y_val_fold, y_pred_proba_fold)\n        auc_scores.append(auc)\n\n        # Print the AUC-ROC score for the current fold\n        print(f'{model_name} Fold {train_index} AUC-ROC: {auc:.4f}')\n        print('\\n' + '='*60 + '\\n')\n\n    # Calculate the average AUC-ROC across all folds\n    avg_auc = sum(auc_scores) / len(auc_scores)\n    model_scores[model_name] = avg_auc\n    print(f'{model_name} Average AUC-ROC: {avg_auc:.4f}')\n    print('\\n' + '='*60 + '\\n')\n\n# Compare the models and choose the best one\nbest_model_name = max(model_scores, key=model_scores.get)\nbest_model_avg_auc = model_scores[best_model_name]\nprint(f'Best Model: {best_model_name} with Average AUC-ROC: {best_model_avg_auc:.4f}')\n'''","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Code Cell Output:\n\nRandom Forest Fold [      1       4       5 ... 9203835 9203836 9203837] AUC-ROC: 0.8319\n\n============================================================\n\nRandom Forest Fold [      0       1       2 ... 9203834 9203836 9203837] AUC-ROC: 0.8320\n\n============================================================\n\nRandom Forest Fold [      0       1       2 ... 9203834 9203835 9203837] AUC-ROC: 0.8318\n\n============================================================\n\nRandom Forest Fold [      0       2       3 ... 9203832 9203835 9203836] AUC-ROC: 0.8326\n\n============================================================\n\nRandom Forest Fold [      0       1       2 ... 9203835 9203836 9203837] AUC-ROC: 0.8321\n\n============================================================\n\nRandom Forest Average AUC-ROC: 0.8321\n\n============================================================\n\nGradient Boosting Fold [      1       4       5 ... 9203835 9203836 9203837] AUC-ROC: 0.8625\n\n============================================================\n\nGradient Boosting Fold [      0       1       2 ... 9203834 9203836 9203837] AUC-ROC: 0.8622\n\n============================================================\n\nGradient Boosting Fold [      0       1       2 ... 9203834 9203835 9203837] AUC-ROC: 0.8621\n\n============================================================\n\nGradient Boosting Fold [      0       2       3 ... 9203832 9203835 9203836] AUC-ROC: 0.8629\n\n============================================================\n\nGradient Boosting Fold [      0       1       2 ... 9203835 9203836 9203837] AUC-ROC: 0.8616\n\n============================================================\n\nGradient Boosting Average AUC-ROC: 0.8623\n\n============================================================","metadata":{"editable":false}},{"cell_type":"code","source":"xgb_data = train_df.copy()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_data.columns = xgb_data.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '').str.replace('>', '')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = list(xgb_data.columns)\nfeature_names","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the features and target variable for xgb\nX1 = xgb_data.drop(['Age', 'Response'], axis=1)\ny1 = xgb_data['Response']\n\n# Split the data\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import KFold\n\n# Initialize the models\nmodels = {\n    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, device='cuda', enable_categorical=True)\n}\n\n# Define the K-fold cross-validation object\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Train and evaluate the models\nmodel_scores = {}\nfor model_name, model in models.items():\n    # Initialize the AUC-ROC scores\n    auc_scores = []\n\n    # Perform K-fold cross-validation\n    for train_index, val_index in kfold.split(X_train1):\n        X_train_fold = X_train1.iloc[train_index]\n        y_train_fold = y_train1.iloc[train_index]\n        X_val_fold = X_train1.iloc[val_index]\n        y_val_fold = y_train1.iloc[val_index]\n\n        # Train the model on the current fold\n        model.fit(X_train_fold, y_train_fold)\n\n        # Make predictions on the validation set\n        y_pred_proba_fold = model.predict_proba(X_val_fold)[:, 1]\n\n        # Evaluate the model on the current fold\n        auc = roc_auc_score(y_val_fold, y_pred_proba_fold)\n        auc_scores.append(auc)\n\n        # Print the AUC-ROC score for the current fold\n        print(f'{model_name} Fold {train_index} AUC-ROC: {auc:.4f}')\n        print('\\n' + '='*60 + '\\n')\n\n    # Calculate the average AUC-ROC across all folds\n    avg_auc = sum(auc_scores) / len(auc_scores)\n    model_scores[model_name] = avg_auc\n    print(f'{model_name} Average AUC-ROC: {avg_auc:.4f}')\n    print('\\n' + '='*60 + '\\n')\n\n# Compare the models and choose the best one\nbest_model_name = max(model_scores, key=model_scores.get)\nbest_model_avg_auc = model_scores[best_model_name]\nprint(f'Best Model: {best_model_name} with Average AUC-ROC: {best_model_avg_auc:.4f}')\n'''","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost Fold [      1       4       5 ... 9203835 9203836 9203837] AUC-ROC: 0.8764\n\n============================================================\n\nXGBoost Fold [      0       1       2 ... 9203834 9203836 9203837] AUC-ROC: 0.8764\n\n============================================================\n\nXGBoost Fold [      0       1       2 ... 9203834 9203835 9203837] AUC-ROC: 0.8762\n\n============================================================\n\nXGBoost Fold [      0       2       3 ... 9203832 9203835 9203836] AUC-ROC: 0.8768\n\n============================================================\n\nXGBoost Fold [      0       1       2 ... 9203835 9203836 9203837] AUC-ROC: 0.8762\n\n============================================================\n\nXGBoost Average AUC-ROC: 0.8764\n\n============================================================\n\nBest Model: XGBoost with Average AUC-ROC: 0.8764","metadata":{"editable":false}},{"cell_type":"code","source":"%pip install cupy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nimport optuna\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport cupy\n\n# Move the data to the GPU\nX_train1_gpu = cupy.asarray(X_train1)\ny_train1_gpu = cupy.asarray(y_train1)\n\n'''\n# Define hyperparameter grid\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.1, 0.5, 1],\n    'n_estimators': [50, 100, 200],\n    'gamma': [0, 0.1, 0.5],\n    'subsample': [0.5, 0.8, 1],\n    'colsample_bytree': [0.5, 0.8, 1],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [0, 0.1, 0.5]\n}\n'''\n\n# Convert CuPy arrays to NumPy arrays\nX_train1_np = cupy.asnumpy(X_train1_gpu)\ny_train1_np = cupy.asnumpy(y_train1_gpu)\n\n'''\n# Perform grid search\ngrid_search = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, random_state=42, device='cuda', enable_categorical=True), param_grid, cv=5, scoring='roc_auc')\ngrid_search.fit(X_train1_np, y_train1_np)\n\n# Print best hyperparameters and score\nprint(\"Best hyperparameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\n'''\n\ndef objective(trial):\n    param = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 3, 7)\n    }\n\n    xgb_model = xgb.XGBClassifier(**param, use_label_encoder=False, random_state=42, device='cuda', enable_categorical=True)\n    scores = cross_val_score(xgb_model, X_train1_np, y_train1_np, cv=5, scoring='roc_auc')\n    return np.mean(scores)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best hyperparameters:\", study.best_trial.params)\nprint(\"Best score:\", study.best_trial.value)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nimport optuna\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport cupy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_param = {'learning_rate': 0.3165452668977133, 'n_estimators': 188, 'gamma': 0.2670568191008615, \n              'max_depth': 6, 'min_child_weight': 8, 'subsample': 0.9707704922792166, \n              'colsample_bytree': 0.822608299221203, 'reg_alpha': 0.4054578655101799, \n              'reg_lambda': 0.988778827463794}\n\nxgb_model = xgb.XGBClassifier(**best_param, use_label_encoder=False, random_state=42, device='cuda', enable_categorical=True)\nxgb_model.fit(X_train1_np, y_train1_np)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.shape","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_dataset.copy()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert categorical variables\ncategorical_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\ntest = pd.get_dummies(test, columns=categorical_features, drop_first=True)\n\n# Normalize/Standardize numerical features\nnumerical_features = ['Annual_Premium', 'Vintage']\nscaler = StandardScaler()\ntest[numerical_features] = scaler.fit_transform(test[numerical_features])\n\ntest.head(10)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Binning Age\ntest['Age_Bin'] = pd.cut(test['Age'], bins=[0, 30, 40, 50, 60, 70, 80], labels=[1, 2, 3, 4, 5, 6])\n\n# Create Insurance History feature\ntest['Insurance_History'] = test['Previously_Insured'] * test['Vintage']\n\n# Use quantile-based binning for normalized Vintage values\nquantile_transformer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile', subsample=None)\ntest['Vintage_Bin'] = quantile_transformer.fit_transform(test[['Vintage']])\n\n# Create a new feature combining vehicle damage status and vehicle age\ntest['Vehicle_Damage_Age'] = test['Vehicle_Damage_Yes'] * (\n    test['Vehicle_Age_< 1 Year'] * 1 + \n    test['Vehicle_Age_> 2 Years'] * 2\n)\n\n# Calculate entropy for Policy_Sales_Channel\nsales_channel_entropy = test['Policy_Sales_Channel'].value_counts(normalize=True)\ntest['Policy_Sales_Channel_Entropy'] = entropy(sales_channel_entropy)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns = test.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '').str.replace('>', '')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1 = test.copy()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test1.dtypes)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode the categorical columns\ntest1_encoded = pd.get_dummies(test1, columns=['Age_Bin'])\n\n# Create a CuPy array from the encoded data\ntest_gpu = cp.asarray(test1_encoded)\n\n# Convert the CuPy array back to a NumPy array\ntest_np = cp.asnumpy(test_gpu)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\ny_pred = xgb_model.predict(test_np)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'Response': y_pred\n})\nsubmission.to_csv('submission1.csv', index=False)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}